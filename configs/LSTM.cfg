[MAIN]
TYPE = "LSTM"

[dataset]
len_dict = 30
single_chain = True
max_sequence_length = 128

[training]
learning_rate_gen = 0.001
learning_rate_discrim = 0.0001
batch_size = 64
epochs = 250
final_batches = 100

[network]
Attention = True
embedding_dim = 128
vocab_size = 30
seq_length = 128
z_dim = 128
gf_dim = 48
df_dim = 36
kernel_height = 1
kernel_width = 3
dilation_rate = 2
pooling = None
num_classes = None

[generator]
hidden_size = 256
num_layers = 3
input_size = 100
embed_length = True
sim_num = 3
len_dict = 25
offset = 5
force_pad = True
similarity_loss = False
similarity_loss_frequency = 50
aug_lambda = 10.0

[discriminator]
reg_flag = False
r1_gamma = 1.0
LSTM = False
embedding_dim = 1
hidden_size = 32
num_layers = 1
hidden_size_1 = 128
hidden_size_2 = 64